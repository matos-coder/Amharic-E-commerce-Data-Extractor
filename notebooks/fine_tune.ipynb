{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed5ce35c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# task 3: Fine-Tuning the Model\n",
    "# ==============================================================================\n",
    "# Now we bring everything together to train the model.\n",
    "\n",
    "# --- Data Collator ---\n",
    "# This helper object creates batches of data for training. It will also\n",
    "# pad our sentences to be the same length within a batch.\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)\n",
    "\n",
    "# --- Evaluation Metrics ---\n",
    "# This function calculates the performance of our model during evaluation.\n",
    "# It computes precision, recall, and F1-score, as required by the assignment.\n",
    "seqeval = evaluate.load(\"seqeval\")\n",
    "\n",
    "def compute_metrics(p):\n",
    "    predictions, labels = p\n",
    "    predictions = np.argmax(predictions, axis=2)\n",
    "\n",
    "    # Remove ignored index (-100) and convert predictions to label strings\n",
    "    true_predictions = [\n",
    "        [id2label[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    true_labels = [\n",
    "        [id2label[l] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "\n",
    "    results = seqeval.compute(predictions=true_predictions, references=true_labels)\n",
    "    return {\n",
    "        \"precision\": results[\"overall_precision\"],\n",
    "        \"recall\": results[\"overall_recall\"],\n",
    "        \"f1\": results[\"overall_f1\"],\n",
    "        \"accuracy\": results[\"overall_accuracy\"],\n",
    "    }\n",
    "\n",
    "# --- Calculate Class Weights ---\n",
    "# To address class imbalance, we calculate weights based on label frequencies.\n",
    "# Less frequent labels will have higher weights.\n",
    "if \"train\" in tokenized_datasets:\n",
    "    label_counts = {}\n",
    "    for example in tokenized_datasets[\"train\"]:\n",
    "        for label_id in example[\"labels\"]:\n",
    "            if label_id != -100: # Ignore padding/special tokens\n",
    "                label_counts[label_id] = label_counts.get(label_id, 0) + 1\n",
    "\n",
    "    total_labels = sum(label_counts.values())\n",
    "    # Ensure all possible label IDs from 0 to len(id2label)-1 are included\n",
    "    # even if they don't appear in the training data (assign a small count or handle carefully)\n",
    "    # For simplicity, we'll use counts from the training data, assuming all relevant\n",
    "    # labels appear at least once. If not, you might need to adjust this.\n",
    "    num_classes = len(id2label)\n",
    "    # Initialize counts for all classes to avoid division by zero if a class is missing\n",
    "    full_label_counts = {i: label_counts.get(i, 0) for i in range(num_classes)}\n",
    "\n",
    "    # Calculate inverse frequency weights\n",
    "    # weight_i = total_labels / (num_classes * count_i) or similar\n",
    "    # A common approach is 1 / frequency, then normalize.\n",
    "    # Using total samples / (num_classes * count) helps scale.\n",
    "    weights = [0.0] * num_classes\n",
    "    for i in range(num_classes):\n",
    "        count = full_label_counts[i]\n",
    "        # Add a small smoothing term to avoid division by zero for unseen labels\n",
    "        weights[i] = total_labels / (num_classes * (count + 1e-5))\n",
    "\n",
    "    import torch\n",
    "    class_weights = torch.tensor(weights, dtype=torch.float)\n",
    "    print(\"\\nCalculated class weights:\")\n",
    "    print(class_weights)\n",
    "else:\n",
    "    class_weights = None\n",
    "    print(\"\\nSkipping class weight calculation as training data is not available.\")\n",
    "\n",
    "# --- Load the Pre-trained Model ---\n",
    "# We load the XLM-Roberta model but tell it we are using it for \"Token\n",
    "# Classification\". We also pass our label mappings so it knows what to predict.\n",
    "# We'll pass the calculated class weights to the model's configuration.\n",
    "config = AutoConfig.from_pretrained(\n",
    "    model_checkpoint,\n",
    "    num_labels=len(id2label),\n",
    "    id2label=id2label,\n",
    "    label2id=label2id,\n",
    ")\n",
    "# Pass class weights to the configuration if they were calculated\n",
    "if class_weights is not None:\n",
    "    # Convert the tensor to a list for JSON serialization\n",
    "    config.class_weights = class_weights.tolist() # Convert tensor to list\n",
    "\n",
    "model = AutoModelForTokenClassification.from_pretrained(\n",
    "    model_checkpoint,\n",
    "    config=config, # Pass the modified configuration\n",
    ")\n",
    "print(f\"\\nModel '{model_checkpoint}' loaded and configured for NER with class weights.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
